{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaf7d9e-577a-4e96-94a7-39e4ff7c7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import glob\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, Dinov2WithRegistersModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482cfc99-a690-4c4f-a8dd-aca60361a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('./examples/**.tif')\n",
    "\n",
    "centroids_array_path = './centroids_6k.npy'\n",
    "centroids_arr = np.load(centroids_array_path).astype(np.float32)\n",
    "cetroid_index = faiss.IndexFlatIP(centroids_arr.shape[1])\n",
    "cetroid_index.add(centroids_arr)\n",
    "\n",
    "colormap_array_path = './color_map_rgb_6k.npy'\n",
    "colormap_arr = np.load(colormap_array_path).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d7ffa2-0257-426c-9afe-6ebda45420ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-with-registers-base\")\n",
    "model = Dinov2WithRegistersModel.from_pretrained(\"facebook/dinov2-with-registers-base\")\n",
    "\n",
    "def process_image(image):\n",
    "    features_shape = (16, 16)\n",
    "\n",
    "    h, w, c = image.shape\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        features_flat = last_hidden_state[:, 1 + model.config.num_register_tokens:, :]\n",
    "        features = features_flat.unflatten(1, features_shape)\n",
    "        features_np = features.detach().cpu().numpy().astype(np.float16)\n",
    "\n",
    "    eh, ew, ec = features_np.shape[1], features_np.shape[2], features_np.shape[3]\n",
    "    kmeans_2c = np.zeros([eh, ew, 3], dtype=np.uint8)\n",
    "    for ih in range(eh):\n",
    "        for iw in range(ew):\n",
    "            pxe = features_np[0, ih, iw, :].reshape(1, ec)\n",
    "            _, kmeans_index = cetroid_index.search(pxe, 1) \n",
    "            color_rgb = colormap_arr[kmeans_index[0][0]]\n",
    "            kmeans_2c[ih, iw] = color_rgb\n",
    "    return kmeans_2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2a7f0c-1fec-4da0-a19e-40fb89180ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 224\n",
    "features_size = 16\n",
    "singlecolor_count_threshold = 768\n",
    "one_feature_in_pixels = sample_size / features_size\n",
    "border_in_features = 3\n",
    "one_border_in_pixels = (border_in_features * one_feature_in_pixels)\n",
    "borders_in_pixels = one_border_in_pixels * 2\n",
    "sample_stride_in_pixels = sample_size - borders_in_pixels\n",
    "sample_stride_in_features = features_size - (2 * border_in_features)\n",
    "\n",
    "\n",
    "def calulate_singlecolor(sample_image, threshold=1):\n",
    "    t = torch.from_numpy(np.mean(sample_image, axis=2))\n",
    "    th, tw = t.shape\n",
    "    ts = torch.nn.functional.pixel_unshuffle(t.reshape(1, 1, th, tw), 7)\n",
    "    ts_min = torch.min(ts, dim=1).values\n",
    "    ts_max = torch.max(ts, dim=1).values\n",
    "    ts_diff = ts_max - ts_min\n",
    "    ts_diff.shape\n",
    "    singlecolor_count = int(torch.sum(ts_diff[0,:,:] < threshold))\n",
    "    return singlecolor_count\n",
    "\n",
    "\n",
    "def process_tif_to_tif(src_fp, dst_fp):\n",
    "    with rasterio.open(src_fp) as src:\n",
    "        profile = src.profile\n",
    "        src_crs = src.crs\n",
    "        height_pixels, width_pixels = src.height, src.width\n",
    "    \n",
    "        h1_list_in_pixels = []\n",
    "        h1 = 0\n",
    "        while True:\n",
    "            if h1 + sample_size > height_pixels:\n",
    "                break\n",
    "            h1_list_in_pixels.append(h1)\n",
    "            h1 += sample_stride_in_pixels\n",
    "        b = ((h1 + sample_size) - height_pixels) // 2\n",
    "        h1_list_in_pixels = [i + b for i in h1_list_in_pixels]\n",
    "        \n",
    "        w1_list_in_pixels = []\n",
    "        w1 = 0\n",
    "        while True:\n",
    "            if w1 + sample_size > width_pixels:\n",
    "                break\n",
    "            w1_list_in_pixels.append(w1)\n",
    "            w1 += sample_stride_in_pixels\n",
    "        b = ((w1 + sample_size) - width_pixels) // 2\n",
    "        w1_list_in_pixels = [i + b for i in w1_list_in_pixels]\n",
    "        \n",
    "        \n",
    "        h1w1_list_in_pixels = []\n",
    "        for h1 in h1_list_in_pixels:\n",
    "            for w1 in w1_list_in_pixels:\n",
    "                h1w1_list_in_pixels.append((h1, w1))\n",
    "        \n",
    "        top_pixel = h1_list_in_pixels[0] + one_border_in_pixels\n",
    "        bottom_pixel = (h1_list_in_pixels[-1] + sample_size) - one_border_in_pixels\n",
    "        \n",
    "        left_pixel = w1_list_in_pixels[0] + one_border_in_pixels\n",
    "        right_pixel = (w1_list_in_pixels[-1] + sample_size) - one_border_in_pixels\n",
    "        \n",
    "        dst_west, dst_north = src.xy(top_pixel, left_pixel)\n",
    "        dst_east, dst_south = src.xy(bottom_pixel, right_pixel)\n",
    "        \n",
    "        dst_height, dst_width = (bottom_pixel - top_pixel) / one_feature_in_pixels, (right_pixel - left_pixel) / one_feature_in_pixels\n",
    "        dst_transform = rasterio.transform.from_bounds(dst_west, dst_south, dst_east, dst_north, dst_width, dst_height)\n",
    "\n",
    "        profile = {\n",
    "            'driver': 'GTiff',\n",
    "            'dtype': 'uint8',\n",
    "            'count': 3,\n",
    "            'height': dst_height,\n",
    "            'width': dst_width,\n",
    "            'compress': 'LZW',\n",
    "            'crs':src_crs,\n",
    "            'transform':dst_transform\n",
    "        }\n",
    "    \n",
    "        with rasterio.open(dst_fp, 'w', **profile) as dst:\n",
    "            for h1, w1 in tqdm(h1w1_list_in_pixels):\n",
    "                sample = src.read(window=((h1, h1 + sample_size), (w1, w1 + sample_size)), boundless=True, fill_value=0)\n",
    "                sample = np.transpose(sample[:3, :], (1, 2, 0))\n",
    "                singlecolor_count = calulate_singlecolor(sample, threshold=2)\n",
    "                if singlecolor_count > singlecolor_count_threshold:\n",
    "                    continue\n",
    "    \n",
    "                fetures_image = process_image(sample)\n",
    "\n",
    "                x, y = src.xy(h1 + one_border_in_pixels, w1 + one_border_in_pixels)\n",
    "                dst_h1, dst_w1 = dst.index(x, y)\n",
    "                window = Window(col_off=dst_w1, row_off=dst_h1, width=sample_stride_in_features, height=sample_stride_in_features)\n",
    "                for b in range(3):\n",
    "                    dst.write(fetures_image[border_in_features:-border_in_features,border_in_features:-border_in_features,b], b+1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd5344b-7131-4169-8ba4-260d789befdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./examples/65dd2b5c7175970001f718ba.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1025/1025 [03:23<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./examples/64a71afc64adbc00012e09b6.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 728/728 [04:07<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./examples/62e90217140bec00067db70d.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2601/2601 [07:56<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fp in file_list:\n",
    "    print(fp)\n",
    "    process_tif_to_tif(fp, './i_' + fp.split('/')[-1])\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
